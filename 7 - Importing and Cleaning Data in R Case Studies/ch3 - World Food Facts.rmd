---
title: "Importing & Cleaning Data in R: Case Studies - World Food Facts"
author: "Guillaume Abgrall"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    css: [default, extra.css]
    includes:
      after_body: ../assets/html/footer.html
---

```{r, echo=FALSE, results='hide', message = FALSE}
# Load the knitr and kableExtra packages
library(knitr)
library(kableExtra)
options(knitr.table.format = "html")
# Load the gapminder package
library(gapminder)
# Load the dpylr package
library(dplyr)
# Load the ggplot2 package as well
library(ggplot2)
theme_set(theme_bw())  # pre-set the bw theme.
```

###Importing the data  

As a person of many talents, it's time to take on a different job: nutrition analysis! Your goal is to analyze the sugar content of a sample of foods from around the world.  

A large dataset called `food.csv` is ready for your use in the working directory. Instead of the usual `read.csv()`, however, you're going to use the faster `fread()` from the `data.table package`. By default, the data will come in as a data table, but since you're used to working with data frames, you can get `fread()` to return one by setting `data.table = FALSE`.  

[Note: In order to make these exercises manageable, we've taken a random subset of the original data. The dataset you'll be working with may not be large enough for `fread()` to make a huge difference, but be aware that there will be times when `read.csv()` just won't cut it.]  

```{r, message = FALSE}
# Load data.table
library(data.table)

# Import food.csv as a data frame: food
food <- fread("../xDatasets/food.csv", data.table = FALSE)

```

```{r}
# View summary of food
sum_food <- as.data.frame(do.call(cbind, lapply(food, summary)))

sum_food %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "left", , font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "#3f7689")

# View head of food
food %>% 
  head()  %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "left", , font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "#3f7689") %>%
  scroll_box(width = "100%", height = "300px")

# View structure of food
str(food, give.attr = FALSE)
```

*Information overload. With datasets this big, it's hard to get a handle on exactly what they contain.*  


###Inspecting variables  

The `str()`, `head()`, and `summary()` functions are designed to give you some information about a dataset without being overwhelming. However, this dataset is so large and has so many variables that even these outputs seemed pretty intimidating!   

The `glimpse()` function from the `dplyr` package often formats information in a more approachable way.  

Yet another option is to just look at the column names to see what kinds of data you have. As you look at the names, pay particular attention to any pairs that look like duplicates.  

```{r}
# Load dplyr
library(dplyr)

# View a glimpse of food
glimpse(food)

# View column names of food
names(food)
```

*This is a little more manageable. Before moving on, scroll through the column names and see if you can find pairs that might be duplicates.*  


###Removing duplicate info  

Wow! That's a lot of variables. To summarize, there's some information on what and when information was added (1:9), meta information about food (10:17, 22:27), where it came from (18:21, 28:34), what it's made of (35:52), nutrition grades (53:54), some unclear (55:63), and some nutritional information (64:159).  

There are also many different pairs of columns that contain duplicate information. Luckily, you have a trusty assistant who went through and identified duplicate columns for you.  

A vector has been created for you that lists out all of the duplicates; all you need to do is remove those columns from the dataset. Don't forget, you can use the - operator to specify columns to omit, e.g.:  

> `my_df[, -3]` # Omit third column

```{r}
# Define vector of duplicate cols (don't change)
duplicates <- c(4, 6, 11, 13, 15, 17, 18, 20, 22, 
                24, 25, 28, 32, 34, 36, 38, 40, 
                44, 46, 48, 51, 54, 65, 158)

# Remove duplicates from food: food2
food2 <- food[,-duplicates]
```


###Removing useless info  

Your dataset is much more manageable already.  

In addition to duplicate columns, there are many columns containing information that you just can't use. For example, the first few columns contain internal codes that don't have any meaning to us. There are also some column names that aren't clear enough to tell what they contain.  

All of these columns can be deleted. Once again, your assistant did a splendid job finding the indices for you.  

```{r}
# Define useless vector (don't change)
useless <- c(1, 2, 3, 32:41)

# Remove useless columns from food2: food3
food3 <- food2[, -useless]
```


###Finding columns  

Looking much nicer! Recall from the first exercise that you are assuming you will be analyzing the sugar content of these foods. Therefore, your next step is to look at a summary of the nutrition information.  

All of the columns with nutrition info contain the character string "100g" as part of their name, which makes it easy to identify them.  

```{r}
library(stringr)

# Create vector of column indices: nutrition
nutrition <- str_detect(names(food3), "100g")

# View a summary of nutrition columns
sum_food3 <- as.data.frame(do.call(cbind, lapply(food3[,nutrition], summary)))

sum_food3 %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "left", font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "#3f7689")
```

*Take a look at the results before moving on. Anything noteworthy about the nutrition data*  


###Replacing missing values  

Unfortunately, the summary revealed that the nutrition data are mostly `NA` values. After consulting with the lab technician, it appears that much of the data is missing because the food just doesn't have those nutrients.  

But all is not lost! The lab tech also said that for sugar content, zero values are sometimes entered explicitly, but sometimes the values are just left empty to denote a zero. A statistical miracle!  

In this exercise, you'll replace all `NA` values with `zeroes` in the `sugars_100g` column and make histograms to visualize the result. Then, you will exclude the observations which have no sugar to see how the distribution changes.  

```{r}
# Find indices of sugar NA values: missing
missing <- is.na(food3$sugars_100g)

# Replace NA values with 0
food3$sugars_100g[missing] <- 0

# Create first histogram
hist(food3$sugars_100g, breaks = 100)

# Create food4
food4 <- food3[food3$sugars_100g > 0, ]

# Create second histogram
hist(food4$sugars_100g, breaks = 100)
```

*Excluding the observations which don't contain any sugar, you can better visualize what the underlying distribution looks like. And now, for something completely different.*  


###Dealing with messy data  

Your analysis of sugar content was so impressive that you've now been tasked with determining how many of these foods come in some sort of plastic packaging. (No good deed goes unpunished, as they say.)  

Your dataset has information about packaging, but there's a bit of a problem: it's stored in several different languages (Spanish, French, and English). This takes messy data to a whole new level! There is no R package to selectively translate, but what if you could just work with the messy data directly?  

You're in luck! The root word for plastic is same in English (plastic), French (plastique), and Spanish (plastico). To get a general idea of how many of these foods are packaged in plastic, you can look through the packaging column for the string "plasti".  

```{r}
# Find entries containing "plasti": plastic
plastic <- str_detect(food3$packaging, "plasti")

# Print the sum of plastic
sum(plastic)
```

