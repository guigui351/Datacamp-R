---
title: "Import data with R - Importing data from the web (Part 1)"
author: "Guillaume Abgrall"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    css: [default, extra.css]
    includes:
      after_body: ../assets/html/footer.html
---

```{r, echo=FALSE, results='hide', message = FALSE}
# Load the knitr and kableExtra packages
library(knitr)
library(kableExtra)
options(knitr.table.format = "html")
# Load the gapminder package
library(gapminder)
# Load the dpylr package
library(dplyr)
# Load the ggplot2 package as well
library(ggplot2)
theme_set(theme_bw())  # pre-set the bw theme.
```

##HTTP  

####Import flat files from the web  

The utils functions to import flat file data, such as `read.csv()` and `read.delim()`, are capable of automatically importing from URLs that point to flat files on the web.  

You must be wondering whether Hadley Wickham's alternative package, `readr`, is equally potent. Well, figure it out in this exercise! The URLs for both a `.csv` file as well as a `.delim` file are already coded for you. It's up to you to actually import the data. If it works, that is...  

```{r}
# Load the readr package
library(readr)

# Import the csv file: pools
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"
pools <- read_csv(url_csv)

# Import the txt file: potatoes
url_delim <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt"
potatoes <- read_tsv(url_delim)

# Print pools and potatoes
pools %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = T, position = "left", , font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "#3f7689")

potatoes %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = T, position = "left", , font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "#3f7689")
```


####Secure importing  

In the previous exercises, you have been working with URLs that all start with `http://.` There is, however, a safer alternative to `HTTP`, namely `HTTPS`, which stands for `HypterText Transfer Protocol Secure`. Just remember this: HTTPS is relatively safe, HTTP is not.  

Luckily for us, you can use the standard importing functions with https:// connections since R version 3.2.2.  

```{r}
# https URL to the swimming_pools csv file.
url_csv <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"

# Import the file using read.csv(): pools1
pools1 <- read.csv(url_csv)

# Load the readr package
library(readr)

# Import the file using read_csv(): pools2
pools2 <- read_csv(url_csv)

# Print the structure of pools1 and pools2
str(pools1)
str(pools2)
```



####Import Excel files from the web  

When you learned about `gdata`, it was already mentioned that `gdata` can handle `.xls` files that are on the internet. `readxl` can't, at least not yet. The URL with which you'll be working is already available in the sample code. You will import it once using `gdata` and once with the `readxl` package via a workaround.  

```{r, message = FALSE}
# Load gdata package - Does not owrk as Perl Strawberry not installed
library(gdata)

# Specification of url: url_xls
url_xls <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls"

# Import the .xls file with gdata: excel_gdata
excel_gdata <- read.xls(url_xls, perl = "C:/myperl/perl/bin/perl.exe")

excel_gdata %>%
  kable(caption = "Latitude from read.xls") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center", , font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "#3f7689") %>%
  scroll_box(width = "100%", height = "300px")
```

```{r}
# Load the readxl and gdata package
library(readxl)

# Specification of url: url_xls
url_xls <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls"

# Download file behind URL, name it local_latitude.xls
#download.file(url_xls, destfile = "local_latitude.xls")

# Import the local .xls file with readxl: excel_readxl
excel_readxl <- read_excel("local_latitude.xls")

excel_readxl %>%
  kable(caption = "Latitude from read_excel") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center", , font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "#3f7689") %>%
  scroll_box(width = "100%", height = "300px")
```

*It appears that `readxl` is not (yet?) able to deal with Excel files that are on the web. However, a simply workaround with `download.file()` fixes this.*  


####Downloading any file, secure or not  

In the previous exercise you've seen how you can read excel files on the web using the `read_excel` package by first downloading the file with the `download.file()` function.  

There's more: with `download.file()` you can download any kind of file from the web, using HTTP and HTTPS: images, executable files, but also .RData files. An RData file is very efficient format to store R data.  

You can load data from an RData file using the `load()` function, but this function does not accept a URL string as an argument. In this exercise, you'll first download the RData file securely, and then import the local data file.  

```{r}
# https URL to the wine RData file.
url_rdata <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData"

# Download the wine file to your working directory
download.file(url_rdata, destfile = "wine_local.RData")

# Load the wine data into your workspace using load()
load("wine_local.RData")

# Print out the summary of the wine data
sum_wine <- data.frame(summary(wine))
x <- c("name", "Variable", "Statistics")
colnames(sum_wine) <- x
sum_wine[, 2:3] %>%
  kable(caption = "Summary statistics of Wine data frame") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center", , font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "#3f7689") %>%
  scroll_box(width = "100%", height = "300px")
```

*Another way to load remote RData files is to use the `url()` function inside `load()`. However, this will not save the RData file to a local file.*  

Remember that you can't directly use a URL string inside load() to load remote RData files. You should use url() or download the file first using download.file().  


####HTTP? httr! (1)  

Downloading a file from the Internet means sending a `GET request` and receiving the file you asked for. Internally, all the previously discussed functions use a GET request to download files.  

`httr` provides a convenient function, `GET()` to execute this GET request. The result is a response object, that provides easy access to the status code, content-type and, of course, the actual content.  

You can extract the content from the request using the `content()` function. At the time of writing, there are three ways to retrieve this content: as a raw object, as a character vector, or an R object, such as a list. If you don't tell `content()` how to retrieve the content through the as argument, it'll try its best to figure out which type is most appropriate based on the content-type.  

```{r}
# Load the httr package
library(httr)

# Get the url, save response to resp
url <- "http://www.example.com/"
resp <- GET(url)

# Print resp
resp

# Get the raw content of resp: raw_content
raw_content <- content(resp, as = "raw")

# Print the head of raw_content
head(raw_content)
```

*The raw content of the response doesn't make a lot of sense, does it? Luckily, the `content()` function by default, if you don't specify the as argument, figures out what type of data you're dealing with and parses it for you.*  


####HTTP? httr! (2)  

Web content does not limit itself to HTML pages and files stored on remote servers such as DataCamp's Amazon S3 instances. There are many other data formats out there. A very common one is JSON. This format is very often used by so-called Web APIs, interfaces to web servers with which you as a client can communicate to get or store information in more complicated ways.  

You'll learn about Web APIs and JSON in the exercises that follow, but some experimentation never hurts, does it?  

```{r}
# Get the url
url <- "http://www.omdbapi.com/?apikey=72bc447a&t=Annie+Hall&y=&plot=short&r=json"
resp <- GET(url)

# Print resp
print(resp)

# Print content of resp as text
print(content(resp, as = "text"))

# Print content of resp
print(content(resp))
```

*The fact that `httr` converts the `JSON` response body automatically to an R list is very convenient.*  


